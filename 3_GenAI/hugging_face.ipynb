{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOIMYdkhUN6QEVe+yO89le1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pearl-yu/mist5400fall2025/blob/main/3_GenAI/hugging_face.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MIST 5400 Lab: Using models from huggingFace\n",
        "\n",
        "Adapted from huggingface example notebooks.\n",
        "\n",
        "By Pearl Yu"
      ],
      "metadata": {
        "id": "vxcPSPOUFgnS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quickstart\n",
        "\n",
        "Transformers is designed to be fast and easy to use so that everyone can start learning or building with transformer models.\n",
        "\n",
        "This quickstart introduces you to Transformers' key features and shows you how to:\n",
        "\n",
        "- load a pre-trained model and run inference with [Pipeline](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.Pipeline)\n",
        "- fine-tune a model with [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer)"
      ],
      "metadata": {
        "id": "86hnsYtsFoSX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# First, install the transformers libraries"
      ],
      "metadata": {
        "id": "wdIZapmlFxaU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformers installation\n",
        "! pip install transformers datasets evaluate accelerate"
      ],
      "metadata": {
        "id": "5GiKsMvrFjiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Pretrained Models\n",
        "\n",
        "The [Pipeline](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.Pipeline) class is the most convenient way to inference with a pretrained model. It supports many tasks such as text generation, image segmentation, automatic speech recognition, document question answering, and more.\n",
        "\n",
        "> [!TIP]\n",
        "> Refer to the [Pipeline](https://huggingface.co/docs/transformers/main/en/./main_classes/pipelines) API reference for a complete list of available tasks.\n",
        "\n",
        "Create a [Pipeline](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.Pipeline) object and select a task. By default, [Pipeline](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.Pipeline) downloads and caches a default pretrained model for a given task. Pass the model name to the `model` parameter to choose a specific model.\n",
        "\n",
        "<hfoptions id=\"pipeline-tasks\">\n",
        "<hfoption id=\"text generation\">\n",
        "\n",
        "Use `Accelerator` to automatically detect an available accelerator for inference."
      ],
      "metadata": {
        "id": "0x727mQAF3v3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Generation"
      ],
      "metadata": {
        "id": "Q4ePRuzRu6-x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQiHHwqohVeV"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "from accelerate import Accelerator\n",
        "\n",
        "device = Accelerator().device\n",
        "\n",
        "pipeline_text = pipeline(\"text-generation\", model=\"distilgpt2\", device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaeKib_YhVeV"
      },
      "source": [
        "Prompt [Pipeline](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.Pipeline) with some initial text to generate more text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOQnbGcDhVeV"
      },
      "outputs": [],
      "source": [
        "pipeline_text(\"The secret to baking a good cake is \", truncation=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image Classification"
      ],
      "metadata": {
        "id": "QuYWQawHtm1r"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ilOAm4khVeV"
      },
      "source": [
        "</hfoption>\n",
        "<hfoption id=\"image segmentation\">\n",
        "\n",
        "Use `Accelerator` to automatically detect an available accelerator for inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "9R0COhZNhVeV"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "from accelerate import Accelerator\n",
        "\n",
        "device = Accelerator().device\n",
        "\n",
        "pipeline = pipeline(\"image-segmentation\", model=\"facebook/detr-resnet-50-panoptic\", device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tar-o-HihVeV"
      },
      "source": [
        "Pass an image - a URL or local path to the image - to [Pipeline](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.Pipeline).\n",
        "\n",
        "<div class=\"flex justify-center\">\n",
        "   <img src=\"https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png\"/>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4f-8cfEhVeV"
      },
      "outputs": [],
      "source": [
        "segments = pipeline(\"https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png\")\n",
        "segments[0][\"label\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transcription Models"
      ],
      "metadata": {
        "id": "63gVJcJftsav"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VphCjJzohVeV"
      },
      "source": [
        "</hfoption>\n",
        "<hfoption id=\"automatic speech recognition\">\n",
        "\n",
        "Use `Accelerator` to automatically detect an available accelerator for inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lj2rQfcohVeV"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "from accelerate import Accelerator\n",
        "\n",
        "device = Accelerator().device\n",
        "\n",
        "pipeline = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-tiny.en\", device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIc7ldHMhVeV"
      },
      "source": [
        "Pass an audio file to [Pipeline](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.Pipeline)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXSuK1XfhVeV"
      },
      "outputs": [],
      "source": [
        "pipeline(\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/1.flac\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwY0uPZ9hVeV"
      },
      "source": [
        "</hfoption>\n",
        "</hfoptions>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGpRkcS6hVeV"
      },
      "source": [
        "# Trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BY8V59MrhVeV"
      },
      "source": [
        "[Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) is a complete training and evaluation loop for PyTorch models. It abstracts away a lot of the boilerplate usually involved in manually writing a training loop, so you can start training faster and focus on training design choices. You only need a model, dataset, a preprocessor, and a data collator to build batches of data from the dataset.\n",
        "\n",
        "Use the [TrainingArguments](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments) class to customize the training process. It provides many options for training, evaluation, and more. Experiment with training hyperparameters and features like batch size, learning rate, mixed precision, torch.compile, and more to meet your training needs. You could also use the default training parameters to quickly produce a baseline.\n",
        "\n",
        "Load a model, tokenizer, and dataset for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9oE4QdmGhVeV"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
        "dataset = load_dataset(\"rotten_tomatoes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kOTKjcNhVeV"
      },
      "source": [
        "Create a function to tokenize the text and convert it into PyTorch tensors. Apply this function to the whole dataset with the [map](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.map) method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fww3xD_1hVeV"
      },
      "outputs": [],
      "source": [
        "def tokenize_dataset(dataset):\n",
        "    return tokenizer(dataset[\"text\"])\n",
        "dataset = dataset.map(tokenize_dataset, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lgqXyRQhVeV"
      },
      "source": [
        "Load a data collator to create batches of data and pass the tokenizer to it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLjUPR8_hVeV"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUFJHG_1hVeV"
      },
      "source": [
        "Next, set up [TrainingArguments](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments) with the training features and hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gat1asVVhVeV"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"distilbert-rotten-tomatoes\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=2,\n",
        "    report_to=[] # Disable logging to any online platforms\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e16lodHThVeV"
      },
      "source": [
        "Finally, pass all these separate components to [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) and call [train()](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.train) to start."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pOBO2W6WhVec"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Tsawjq4hVec"
      },
      "source": [
        "Congratulations, you just trained your first model with Transformers!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tI-1P-9MhVec"
      },
      "source": [
        "# Next steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJwgrDRihVec"
      },
      "source": [
        "Now that you have a better understanding of Transformers and what it offers, it's time to keep exploring and learning what interests you the most.\n",
        "\n",
        "- **Base classes**: Learn more about the configuration, model and processor classes. This will help you understand how to create and customize models, preprocess different types of inputs (audio, images, multimodal), and how to share your model.\n",
        "- **Inference**: Explore the [Pipeline](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.Pipeline) further, inference and chatting with LLMs, agents, and how to optimize inference with your machine learning framework and hardware.\n",
        "- **Training**: Study the [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) in more detail, as well as distributed training and optimizing training on specific hardware.\n",
        "- **Quantization**: Reduce memory and storage requirements with quantization and speed up inference by representing weights with fewer bits.\n",
        "- **Resources**: Looking for end-to-end recipes for how to train and inference with a model for a specific task? Check out the task recipes!"
      ]
    }
  ]
}